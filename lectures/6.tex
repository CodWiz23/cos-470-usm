%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 6
%            : University of Southern Maine 
%            : @james.quinlan
%            : Cody Savage
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Perceptrons Continued
    \1 Introduction to SVM
    \1 XPLs
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------

\section{Perceptrons Continued}
The Perceptron is part of a family of machine learning algorithms known as \textit{supervised learning} algorithms.

\subsection{Supervised vs Unsupervised Learning Algorithms}
A \textit{supervised learning} algorithm is one which takes in labeled data, from which relationships between the desired output corresponding to the desired inputs are already known, from which the algorithm can make predictions on new, unlabeled data.

Meanwhile, an \textit{unsupervised learning} algorithm is one that takes in unlabeled data, from which it is expected to find relationships and patterns among the data. For our purposes right now, let's focus on the different categories of \textit{supervised learning} algorithms.

\subsection{Types of Supervised Learning Algorithms}
The two primary types of supervised learning algorithms are \textit{regression} algorithms, and \textit {classification} algorithms.

\subsubsection{Regression Algorithms}
Regression algorithms, given an input $x$, maps it to a $y$, such that
\[
    y \in \mathbb{R}.
\]
That is, $x$ is mapped to a real number.

\subsubsection{Classification Algorithms}
Classification algorithms, given an input $x$, maps it to a $y$, such that
\[
    y \in \{C_1, C_2, \dots, C_n\}.
\]
That is, $x$ is mapped to $C_i$, out of $n$ possible categories.
Coincidentally, the Perceptron algorithm belongs to this category. More specifically, it is a \textit{binary classification} algorithm, where 
\[
    y \in \{-1, 1\}.
\]

\subsection{Perceptrons and Linear Separability}
The Perceptron algorithm makes the assumption that data it takes in is \textit{linearly separable}.

A set of data $D$, such that
$
    D = \{(x_1,y_1), \dots,(x_n, y_n)\},
$, $
    x_i \in \mathbb{R}^p,
$, and $
    y \in \{-1, 1\}.
$ is \textit{linearly separable} if and only if one can draw a hyperplane between the two sets, such that
\[
    y_i(w^\perp x +b) > 0,
\] \[
    \forall x_i, y_i.
\]
Recall that it is the sign of $w^\perp x+b$ that tells us whether or not vector $x$ is 'above', 'below', or lying in the hyperplane. Assuming the hyperplane obtained via the Perceptron is a solution, then the sign should match the sign of the category, so if we were to multiply the category with the equation, we'll either be multiplying two positive or negative numbers together, and we should therefore always obtain a positive number.

\subsection{The Set of all Decision Functions}
Assuming that the set given to it is linearly separable, then the Perceptron algorithm is guaranteed to obtain \textit{a solution}, a hyperplane successfully splits the two classes. However, this solution is not guaranteed to be the \textit{optimal solution}, as we can see here. Consider the following dataset and decision boundary:

\begin{tikzpicture}
	\begin{axis}[
		xlabel=$x$,
		ylabel=$y$,
        xmin=-1,
        xmax=1,
        ymin=-1,
        ymax=1
	]
	\addplot [color=blue, only marks, mark=+] coordinates { 
    (-0.5, 0.5)
    (-0.2, 0.25)
    (0.75, 0.5)
    };
    \addplot [color=red, only marks, mark=-] coordinates { 
    (0.5, -0.5)
    (0, -0.6)
    (0.25, -0.75)
    };
    \addplot[no marks] coordinates {
		(-0.5, -1)
		(1, 0.6)
	};
	\end{axis}
\end{tikzpicture}

As it can be seen, while the feature vectors in the dataset are correctly categorized, the way the plane lies leaves very little room for error, and appears prone to misclassifications.

\begin{tikzpicture}
	\begin{axis}[
		xlabel=$x$,
		ylabel=$y$,
        xmin=-1,
        xmax=1,
        ymin=-1,
        ymax=1
	]
	\addplot [color=blue, only marks, mark=+] coordinates { 
    (-0.5, 0.5)
    (-0.2, 0.25)
    (0.75, 0.5)
    };
    \addplot [color=red, only marks, mark=-] coordinates { 
    (0.5, -0.5)
    (0, -0.6)
    (0.25, -0.75)
    };
    \addplot[no marks] coordinates {
		(-1, -0.5)
		(1, 0)
	};
	\end{axis}
\end{tikzpicture}

Meanwhile, this hyperplane splits the two halves of the feature space way more evenly, leaving more of a margin between the closest feature vectors to the plane. There is an infinite number of possible solutions, an infinite number of possible hyperplanes, and therefore an infinite number of possible \textit{decision functions}, which take the form
\[
    h(x) = sign(w^\perp x+b).
\]
If the hyperplane is the set of all points such that $w^\perp x+b=0$, or, the set of all points in the hyperplane, then the sign of $w^\perp x+b$ when non-zero tells us whether or not we are 'above' or 'below' the plane, and as a result, which category we belong to. From this we can define the set of all possible decision functions,
\[
    H = \{h_1, h_2, \dots, h_n\}.
\]

\section{Introduction to SVM}
As could be seen in the previous section, it appears that an 'optimal' hyperplane is one which maximizes the margin between the closest feature vector and the plane. To better maximize this margin, we'll introduce a new machine learning algorithm called the \textit{Support Vector Machine}, or \textit{SVM} for short.

The SVM is what is known as a \textit{maximum margin classifier}, which means that it is \textit{guaranteed} to find the maximum margin separating hyperplane, which is what we have decided to define as the most 'optimal' hyperplane for our purposes.

\subsection{SVM Prerequisites}
Before we delve into the specifics of the SVM algorithm, we need to define an important linear algebra operation: \textit{projection}.

In the most literal sense, 'projecting' one vector onto another gives us the magnitude of the first vector in the direction of another, as if you sat the first above the second in the same plane and shined a light over it- the projection is it's 'shadow' (like the example below).

\begin{tikzpicture}
	\draw[->] (0, 0) -- (3, 0);
    \draw[blue, ->] (0, 0) -- (1.5, 1);
    \draw[red, ->] (0, 0) -- (1.5, 0);
\end{tikzpicture}

Let $u$ and $v$ be two vectors of some kind, and lets project the first onto the second. If a vector can be defined as a unit direction vector multiplied by some magnitude, we can take the direction of the resulting projection to be the unit vector of the vector we're projecting onto, $\hat{v}$ in this case.

To obtain the magnitude, let us examine the definition for the dot product.
\[
    u \cdot v = |u||v|cos\theta
\]
The dot product can be defined as the magnitudes of both vectors, multiplied by the cosine of the angle between them. Dividing both sides by $|v|$, we obtain
\[
    \frac{u \cdot v}{|v|} = |u|cos\theta.
\]
As it so happens, $|u|cos\theta$ represents the magnitude of $u$ in the direction of $v$. For example, if $u$ and $v$ are parallel, the angle between them is zero, the cosine of zero is 1, and we get the full magnitude of $u$. If $u$ and $v$ are perpendicular, the angle between them is $\frac{\pi}{2}$, and the cosine of that is zero, so there is no magnitude of $u$ in the direction of $v$.

Putting both together, we can define projection to be
\[
    proj_vu = (\frac{u \cdot v}{|v|})\hat{v} = (\frac{u \cdot v}{|v|})\frac{v}{|v|}.
\]

From this, we can also obtain the component of $u$ \textit{perpendicular} to $v$, $u^\perp$, by subtracting the projection from $u$,
\[
    u^\perp = u - proj_vu.
\]
% ----------------------------------------------------------------

\section{XPL's}
1.) Modify perception code to add a counter to see how many updates it takes to obtain any solution.