%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 15 
%            : Optimizers
%            : University of Southern Maine 
%            : @james.quinlan
%            : Gabrielle Akers - Lecture 15
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Optimizers
    \1 Batch Gradient Descent
    \1 Momentum
    \1 RMSProp
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------
\section{Optimizers}
\begin{enumerate}
    \item Batch Gradient Descent
    \begin{itemize}
        \item In the past couple of classes we have been focusing primarily on Batch Gradient Descent and the math behind it.
    \end{itemize} 
    \textbf{Issues with Batch Gradient Descent}
    \begin{enumerate}
        \item Computationally expensive
        \begin{itemize}
            \item Batch gradient descent computes the gradient at every single data point, which ensures points don't get passed over, but also requires a lot of needless computations
        \end{itemize} 
        \item Notational issues
        \[
        loss = l(w,x)
        \]
        \[
        L(w,x) = \frac{1}{n} \sum l(w,x)
        \]
        \[
        \nabla_w L(w,x)=\nabla (\frac{1}{n} \sum l(w,x)) = \frac{1}{n} \sum \nabla l(w,x)
        \]
        The derivative of a sum should be equal to the sum of the derivatives
        \item Slow convergence
        \begin{itemize}
            \item The steps get progressively smaller
        \end{itemize}
        \item Gets stuck on local minimums
    \end{enumerate}
    \textbf{Related Types of Gradient Descent}
    \begin{enumerate}
        \item Stochastic Gradient Descent (SGD)
        \begin{itemize}
            \item This type of gradient descent selects a random x every time
            \[
            l(w,x_r)
            \]
        \end{itemize}

        \item Minibatch Gradient Descent
        \begin{itemize}
            \item A combination of SGD and Batch Gradient Descent. It uses a sample of x values and selects a random value from that sample for computation.
            \item The general rule for the batch size is 32 or less.
        \end{itemize} 
        \[
        batchsize \leq 32
        \]
    \end{enumerate}
    \item Momentum
    \begin{itemize}
        \item  A type of gradient descent that takes into consideration previous gradients in order to prevent getting stuck at local minimums.
        \[
        w_{t+1}=w_t-\alpha \nabla_w L(w_t)-previous\ gradients
        \]
    \end{itemize}
    \textbf{Features of Momentum}
    \begin{enumerate}
        \item Faster convergence than Batch Gradient Descent
        \item Doesn't get stuck at local minimums
        \item The amount of past gradients used are controlled with exponentially decaying weighted averages
        \[
        v_{t+1}=\beta v_t + (1-\beta)\nabla_wL(w_t)
        \]
        \[
        w_{t+1}=w_t-\alpha v_{t+1}
        \]
        Hyperparameters
        \[
        \alpha = learning\ rate
        \]
        \[
        \beta = weights
        \]
        \item Hyperparameters are tuned with cross validation, as shown in Figure \ref{fig:data_split_extended}, which was originally created in Lecture 6.
        % \begin{align}
 \begin{figure}[ht]
    \centering
    \begin{tikzpicture}[node distance=1.5cm and 2cm, auto]
        % Nodes
        \node (data) [draw, rectangle] {Data};
        \node (train) [below left=of data, draw, rectangle] {Training Set};
        \node (test)  [below right=of data, draw, rectangle] {Test Set};
        \node (dev)   [below left=of train, draw, rectangle] {Dev Set};
        \node (val)   [below right=of train, draw, rectangle] {Validation Set};
        \node (build) [below=of dev, draw, rectangle] {Develop, build, or train model};
        \node (train_all) [below=of val, draw, rectangle] {Train on everything};
        \node (final_test) [below=of train_all, draw, rectangle] {Finally test/retrain if test was good};
        
        % Arrows
        \draw[->] (data) -- (train);
        \draw[->] (data) -- (test);
        \draw[->] (train) -- (dev);
        \draw[->] (train) -- (val);
        \draw[->] (dev) -- (build);
        \draw[->] (val) -- (train_all);
        \draw[->] (build) -- (train_all);
        \draw[->] (train_all) -- (final_test);
        \draw[->, bend left=45] (test) to (final_test);
    \end{tikzpicture}
    \caption{Lecture 6 - Model Training Process}
    \label{fig:data_split_extended}
\end{figure}
        % \end{align}
        \begin{itemize}
            \item The best practice has $\beta =0.9$, but it just has to be less than 1
        \end{itemize} 
        \item Update function
        \[
        w_{t+1} = \sum_{i=0}^{t} \beta^i(1-\beta)\nabla L_{t-i}
        \]
    \end{enumerate}
    \textbf{Issues with Momentum}
    \begin{enumerate}
        \item Can overshoot and miss the global minimum
        \item Still can get stuck at some local minimums
    \end{enumerate}
    Unrolling the recurrence relation
    \[
    v_{t+1} = \beta v_t+(1-\beta)\nabla L_t
    \]
    \[
    v_{t+1} = \beta (\beta v_{t-1} +(1-\beta)\nabla L_{t-1})+(1-\beta)\nabla L_t
    \]
    \[
    v_{t+1}=\beta^2v_{t-1}+\beta(1-\beta)\nabla L_{t-1} +(1-\beta)\nabla L_t
    \]
    \[
    v_{t+1}=\beta^2(\beta v_{t-2}+(1-\beta)\nabla L_{t-2})+\beta(1-\beta)\nabla L_{t-1}+(1-\beta)\nabla L_t
    \]
    \[
    v_{t+1}=\beta^3v_{t-2}+\beta^2(1-\beta)\nabla L_{t-2}+\beta(1-\beta)\nabla L_{t-1}+(1-\beta)\nabla L_t
    \]
    \item RMSProp
    \begin{itemize}
        \item A type of gradient descent that is adaptable depending on the size of the gradient
    \end{itemize}
    \begin{align*}
    \nabla L(w) &= \begin{bmatrix}
           \delta L/\delta w_1    \\
           \delta L\delta w_2    \\
           \vdots   \\
           \delta L/\delta w_n    \\
         \end{bmatrix} \\
    \end{align*}
    \textbf{Features of RMSProp}
    \begin{enumerate}
        \item Weighs gradients differently in different directions
        \begin{enumerate}
            \item Large gradient $\rightarrow$ Small step
            \item Small gradient $\rightarrow$ Large step
        \end{enumerate}
        \item Learning rates are different for each component
        \item Element wise operations
        \[
        \hspace{1.1cm} \rightarrow cross product \rightarrow vector
        \]
        \[
        uxv \rightarrow dot product \rightarrow scalar
        \]
        \[
        \hspace{2.3cm} \rightarrow Hadamard\ product \rightarrow vector
        \]
    \end{enumerate}
    \item AdaGrad
    \item NAG (Nestrov Accelerated Gradient)
    \item Adam
    \begin{itemize}
        \item Created in 2015, Adam is generally considered to be the best type of gradient descent. It takes the best features of both Momentum and RMSProp and combines them into a single type of gradient descent.
    \end{itemize}
\end{enumerate}
