%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Support Vectors and Multivariable Calculus 
%            : University of Southern Maine 
%            : @james.quinlan
%            : SJ Franklin - Lecture 10
%            : Updated by Abdirahman Mohmaed
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{outline}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{hyperref}
\title{Support Vectors and Multivariable Calculus}
\date{Lecture 10}

\begin{document}
\maketitle

\section*{Objectives}
\begin{itemize}
    \item Define the SVM margin and its significance
    \item State the primal SVM objective
    \item Describe support vectors
    \item Introduce Lagrange multipliers
    \item Review gradient descent methods
\end{itemize}

\hrulefill

\section{Margin \& Support Vectors}
Margin = distance from hyperplane to nearest points (support vectors). Maximizing margin improves generalization and reduces overfitting.

\section{Primal Objective}
\[
\min_{w,b}\;\frac{1}{2}\|w\|^2
\quad\text{subject to}\quad
y_i(w^T x_i + b)\ge1\quad\forall i.
\]

\section{Support Vectors}
Points satisfying $y_i(w^T x_i + b)=1$ determine the hyperplane; all other multipliers are zero.

\section{Lagrange Multipliers}
Introduce $\alpha_i\ge0$ to enforce constraints:
\[
L(w,b,\alpha)
= \frac{1}{2}\|w\|^2
- \sum_{i=1}^n \alpha_i\bigl[y_i(w^T x_i + b)-1\bigr].
\]

\section{Weighted Sum Representation}
\[
w = \sum_{k=1}^K \alpha_k\,y_k\,x_k.
\]

\section{Gradient Descent}
Iterative update:
\[
w^{(t+1)} = w^{(t)} - \eta\,\nabla_w L(w^{(t)},b^{(t)},\alpha).
\]

\section{Illustration}
\begin{figure}[h]
\centering
\begin{tikzpicture}[scale=1.25]
  % Original TikZ code for data points, hyperplane and margins
  % â€¦ (omitted for brevity)
\end{tikzpicture}
\caption{Optimal hyperplane and maximum margins.}
\label{fig:maxmargin-svm}
\end{figure}

\end{document}
