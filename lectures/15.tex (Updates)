%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Calculus Review and Backpropagation
%            : University of Southern Maine 
%            : @james.quinlan
%            : Michael Yattaw - Lecture 15 (4/16/2025)  
%            : Abdirahman Mohamed (Updated the notes)
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{outline}
\usepackage{tcolorbox}
\usepackage{hyperref}
\title{Calculus Review and Backpropagation}


\begin{document}
\maketitle

\section*{Objectives}
\begin{outline}
    \1 Review function types and derivative fundamentals
    \1 Derive key rules: power, product, quotient, chain
    \1 Introduce partial derivatives and gradients
    \1 Formulate backpropagation via chain rule
    \1 Survey gradient‐descent variants for training
\end{outline}

\hrulefill

\section{Function Types and Domains}
\begin{itemize}
    \item $f: \mathbb{R}\to\mathbb{R}$: single‐variable
    \item $f: \mathbb{R}^n\to\mathbb{R}$: multivariable→scalar
    \item $f: \mathbb{R}^n\to\mathbb{R}^m$: multivariable→vector
\end{itemize}

\section{Definition of Derivative}
For $y=f(x)$:
\[
f'(x) = \lim_{h\to0}\frac{f(x+h)-f(x)}{h}.
\]
\subsection*{Exercises}
1.\ If $f(x)=x^2$, show $f'(x)=2x$.  
2.\ If $g(x)=C$, show $g'(x)=0$.

\section{Differentiation Rules}
\paragraph{Power Rule}
\[
\frac{d}{dx}x^n = n x^{n-1}.
\]
\paragraph{Product Rule}
\[
\frac{d}{dx}[f(x)g(x)] = f'(x)g(x) + f(x)g'(x).
\]
\paragraph{Quotient Rule}
\[
\frac{d}{dx}\!\Bigl(\frac{f}{g}\Bigr)
= \frac{f'g - fg'}{g^2}.
\]
\paragraph{Chain Rule}
\[
\frac{d}{dx}f(g(x)) = f'(g(x))\,g'(x).
\]

\section{Partial Derivatives \& Gradients}
For $f(x,y)=x^3+3y^2$:
\[
\frac{\partial f}{\partial x}=3x^2,\quad
\frac{\partial f}{\partial y}=6y,\quad
\nabla f=\begin{bmatrix}3x^2\\6y\end{bmatrix}.
\]

\section{Backpropagation}
Backprop uses the chain rule to compute $\tfrac{\partial L}{\partial w}$ layer by layer.

\begin{tcolorbox}[title=Weight Update]
Given loss $L$ and weight $w_{ij}$,
\[
w_{ij}\leftarrow w_{ij} - \eta\,\frac{\partial L}{\partial w_{ij}}.
\]
\end{tcolorbox}

\section{Gradient Descent Variants}
\begin{itemize}
  \item Vanilla (full batch)
  \item Stochastic (one sample)
  \item Mini‐Batch
  \item Momentum (with velocity)
\end{itemize}

\end{document}
