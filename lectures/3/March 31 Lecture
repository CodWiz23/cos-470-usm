\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\title{COS 470 Notes}
\author{Mohamed Noor}
\date{March 2025}

\begin{document}
\section*{COS 470 \hfill 3/31/25}
\subsection*{Wrapping Up: Support Vector Machines (SVM) â€” Separable Case}

\begin{itemize}
    \item Goal: Define and optimize the decision function $f(x)$.
    \item Optimization leads to finding parameters $w$ and $b$.
    \item Direct analytical methods usually run into issues (e.g., matrix inversion problems).
    \item So, we turn to gradient-based optimization methods.
\end{itemize}

\vspace{1em}

\subsection*{Objective Function}

The dual form of the SVM optimization problem is:

\[
f(\bm{\alpha}) = \sum_{i=1}^n \alpha_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \alpha_i \alpha_j y_i y_j \bm{x}_i^\top \bm{x}_j
\]

We define matrix $Q$ such that:

\[
Q_{ij} = y_i y_j \bm{x}_i^\top \bm{x}_j
\]

So the optimization problem becomes:

\[
f(\bm{\alpha}) = \bm{1}^\top \bm{\alpha} - \frac{1}{2} \bm{\alpha}^\top Q \bm{\alpha}
\]

\vspace{1em}

\subsection*{Gradient of the Objective}

\[
\nabla f(\bm{\alpha}) = \bm{1} - Q \bm{\alpha}
\]

Or element-wise:

\[
\nabla f(\bm{\alpha})_k = 1 - \sum_{i=1}^n \alpha_i y_i y_k \bm{x}_i^\top \bm{x}_k = 1 - y_k \left( \sum_{i=1}^n \alpha_i y_i \bm{x}_i^\top \bm{x}_k \right)
\]

Setting the gradient to zero gives:

\[
1 - \sum_{i=1}^n \alpha_i y_i y_k \bm{x}_i^\top \bm{x}_k = 0
\quad \Rightarrow \quad
Q \bm{\alpha} = \bm{1}
\]

\subsection*{Matrix Representation}

The full system of equations:

\[
\begin{bmatrix}
    y_1 y_1 \bm{x}_1^\top \bm{x}_1 & y_1 y_2 \bm{x}_1^\top \bm{x}_2 & \cdots & y_1 y_n \bm{x}_1^\top \bm{x}_n \\
    y_2 y_1 \bm{x}_2^\top \bm{x}_1 & y_2 y_2 \bm{x}_2^\top \bm{x}_2 & \cdots & y_2 y_n \bm{x}_2^\top \bm{x}_n \\
    \vdots & \vdots & \ddots & \vdots \\
    y_n y_1 \bm{x}_n^\top \bm{x}_1 & y_n y_2 \bm{x}_n^\top \bm{x}_2 & \cdots & y_n y_n \bm{x}_n^\top \bm{x}_n
\end{bmatrix}
\begin{bmatrix}
    \alpha_1 \\
    \alpha_2 \\
    \vdots \\
    \alpha_n
\end{bmatrix}
=
\begin{bmatrix}
    1 \\
    1 \\
    \vdots \\
    1
\end{bmatrix}
\]

\subsection*{Direct Solution}

\[
\bm{\alpha} = Q^{-1} \bm{1}
\]

\textbf{Problems with direct inversion:}
\begin{itemize}
    \item Taking the inverse of a matrix is numerically unstable.
    \item Computationally expensive, especially for large $n$.
    \item Can't always trust results from the inverse due to rounding errors or singularity.
\end{itemize}

\subsection*{Iterative Methods: Gradient Descent}

To avoid direct inversion, we use gradient-based optimization.

\vspace{1em}

\textbf{Gradient Descent Update Rule:}
\[
\bm{\alpha}_{\text{new}} = \bm{\alpha} - \eta \cdot \nabla f(\bm{\alpha})
\quad \text{where } \eta \text{ is the learning rate}
\]

\vspace{0.5em}

\textbf{Learning rate:} $\eta$ is a hyperparameter ( $\approx 0.1$ or $0.01$) that controls the step size of each update.

\end{document}
