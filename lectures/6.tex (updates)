%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Regression and Loss Functions Review
%            : University of Southern Maine 
%            : @james.quinlan
%            : Ellis Fitzgerald - Lecture 6
%            : Updated by Abdirahman Mohamed
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath,amssymb}
\usepackage{outline}
\usepackage{verbatim}
\usepackage{hyperref}
\title{Regression and Loss Functions Review}


\begin{document}
\maketitle

\section*{Objectives}
\begin{outline}
    \1 Define regression vs.\ classification
    \1 Compare loss and cost functions
    \1 Introduce regularization techniques
    \1 Review gradient descent \& learning‐rate trade-offs
    \1 Recap Lectures 1–8 core concepts
    \1 Work through practice problems
\end{outline}

\hrulefill

\section{Regression vs.\ Classification}
Classification labels: $y\in\{-1,1\}$ or $\{1,2,3\}$.  
Regression labels: $y\in\mathbb{R}$.  
Dataset: $D=\{(x_i,y_i)\}_{i=1}^n$.

\section{Loss \& Cost Functions}
\subsection{Zero–One Loss}
\[
l_{01}(y,\hat y)=
\begin{cases}
0,&y=\hat y,\\
1,&y\neq\hat y.
\end{cases}
\]
Cost: $\sum_i l_{01}(y_i,\hat y_i)$.

\subsection{Absolute Loss}
\[
\text{Loss}=|y_i-\hat y_i|,\quad
\text{Cost}=\sum_i|y_i-\hat y_i|.
\]

\subsection{Square Loss}
\[
\text{Loss}=(y_i-\hat y_i)^2,\quad
\text{Cost}=\sum_i(y_i-\hat y_i)^2.
\]

\section{Objective Function \& Regularization}
\[
\text{Objective}=\text{Cost} + \lambda\,R(w),
\]
with $R(w)=$ L1 or L2 penalty to discourage large weights.

\section{Gradient Descent}
Variants: Vanilla, Stochastic, Mini‐Batch, Hessian, Momentum.  
Learning rate $\eta$:  
\begin{itemize}
    \item Too large → overshoot  
    \item Too small → slow convergence
\end{itemize}

\section{Review Lectures 1–7}
\subsection{Perceptron}
Rosenblatt (1957):  
Update rule $w\leftarrow w + \alpha\,y_i x_i$ for linear separability.

\subsection{KNN}
$k$‐nearest neighbors: compute distances, sort, then vote/regress.

\begin{verbatim}
w=0; m=1;
while m>0:
    m=0
    for i in 1..n:
        if y[i]*dot(w,x[i])<=0:
            w=w+alpha*y[i]*x[i]
            m=m+1
        end
    end
end
\end{verbatim}

\section{Practice Problems}
\begin{enumerate}
    \item What benefits arise from reducing dimensionality?
    \item List types of supervised ML.
    \item Define a loss function.
    \item How does KNN differ in classification vs.\ regression?
    \item What is an SVM and its goal?
    \item Define a distance function and give examples.
\end{enumerate}

\end{document}
