%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
% SUMMARY    : Lecture 5
%            : University of Southern Maine 
%            : @james.quinlan
%            : Courtney Jackson
%+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

\section*{Objectives}
\begin{outline}
    \1 Questions from last lecture
    \1 More about kNN
    \1 Code in Julia
    \1 XPL's
\end{outline}

\rule[0.0051in]{\textwidth}{0.00025in}
% ----------------------------------------------------------------


\section{Questions from Last Lecture}
Going back to two problems from last class

1.) $[a,b] \rightarrow [0,1]$ mapping interval

2.) $[a,b] \rightarrow [c,d]$ \\

1.) We want to map $a \rightarrow 0$ and $b \rightarrow 1$ \\
First we need to come up with a formula
\[
    x=\frac{1}{b-a}(x-a) or \frac{1}{b-a}x-\frac{a}{b-a}
\]
Then we check with values to make sure our formula is correct.
Try $0,\frac{1}{2},1$ we choose these values because it is the two bounds and the half way point between the bounds. \\

effectively you have $(a,0) \land (b,1)$. Where $m=\frac{1-0}{b-a}=\frac{1}{b-a}$ \\

Then you can get $y-0=\frac{1}{b-a}(x-a)= \bold{\frac{1}{b-a}x-\frac{a}{b-a}}$\\

2.) $\frac{x-a}{b-a}d + \frac{b-x}{b-a}c$ \\
$(a,c) \land (b,d) \rightarrow m = \frac{d-c}{b-a}$ \\

% ----------------------------------------------------------------
\section{More about kNN}
argmax$\sum_{i \in n_k}^{}$I$c_j(y_i)$ $j={1,2,...m}$ \\
suppose $j=2$ and $k=8$, it is binary classification, $N_k=$ set of indicies of the KNN \\

Example problem: \\
$y=[1,-1,-1,1,1,1,-1,1]$ $c_1=1,c_2=-1$ $x_t \rightarrow 1$ \\
$argmax_{j=[1,2]}$$[I(1=1)+I(1=-1)+I(1=-1)+I(1=1)+I(1=1)+I(1=1)+I(1=-1))$ \\
 + I(1=1), \\
$I(-1=1)+I(-1=-1)+I(-1=-1)+I(-1=1)+I(-1=1)+I(-1=1)+I(-1=-1)+I(-1=1)$ \\
$argmax_{j=[1,2]} (1+0+0+1+1+1+0+1, 0+1+1+0+0+0+1+0)$\\
$argmax_{j=[1,2]}$ $[5,3]=1$

% ----------------------------------------------------------------

\section{Code in Juila}
The following is code written in Julia 1.10.4. 
It is represeting a kNN algorithm on the Iris dataset.

\begin{lstlisting}
using pkg
usign Distances
using RDataSets
using MLJBase
pkg.add("statsBase")
using StatsBase

iris=dataset("datasets","Iris")
x = Matrix(iris[:,1:4]'
y=@.ifelse(iris.speies=="setosa",1,-1)
c = unique(y)
c[argmax(map(i->sum(y.==c[i]),1:lastindex(c)))]
mode(y)

#This can also be done using the NearestNeighbors package
function kNN(X,x,y,k,d = Euclidean())
    n-size(X,2)
    distances = map(i-> d(x,X[:,i]), 1:n)
    indeices = partialsortperm(distances,1:k)
    if typeof(y) == Vector{union{Float32,Float64}}
        yhat = mean(y[indices])
    else
        yhat = mode
    end
    return yhat
end

#Metrics
accuracy = sum(yhat.==ytest/length(ytest)
precsion = sum ((yhat.==1.&(ytest.==1)))/sum(yhat.==1)
recall = sum((yhat.==1.&(ytest.==1)))/sum(ytest.==1)
specircity = sum((yhat.==-1).&(ytest.==-1))/sum(ytest.==-1)
f1 = 2*[precsion * recall / (precision + recall)
Distances.Hamming()(ytest,yhat)
train, test = parition(1:size(X,2),0.7,shuffle=true)

    Xtrain = X[:,train]
    Xtest = X[:test]
    ytrain = y[train]
    ytest = y[test]

yhat = map(i->kNN(Xtrain,Xtest[:,i],ytrain,1)1:size(Xtest,2))

\end{lstlisting}

The following is some code about setting a random seed
\begin{lstlisting}
pkg.add("random")
using Random
Random.seed!(123) #Where 123 is the seed
\end{lstlisting}

% ----------------------------------------------------------------

\section{XPL's}
1.) Code up a partition function
